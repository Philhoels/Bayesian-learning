---
title: "Computer Lab 1"
author: "Phillip Hölscher"
date: "29 3 2019"
output: 
  pdf_document:
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# libraries for this lab 
library(ggplot2)
library(gridExtra)

```


# 
1. Bernoulli ... again.

Let $y_1,...,y_n \mid \theta \sim Bern(\theta)$ , and assume that you have obtained a sample with s = 14 successes in n = 20 trials. Assume a Beta($\alpha_0$, $\beta_0$) prior for $\theta$ and let $\alpha_0$ = $\beta_0$= 2.

## (a) Draw random numbers from the posterior
$\theta \mid y \sim Beta(\alpha_0 +s, \beta_0+f)$, $y = (y_1,...,y_n)$, and verify graphically that the posterior mean and standard deviation converges to the true values as the number of random draws grows large.
```{r, echo=FALSE}
# calcualte true beta mean and variacne
# initialize given values
alpha0 = 2
beta0 = 2
n = 20
s = 14
f = n - s
alpha_new = alpha0 + s
beta_new = beta0 + f
```



First of all do we have to calcualte the true mean and true standard deviation of the given Beta($\alpha_0$, $\beta_0$).

```{r}
# calculate the true mean and standard deviation
true_mean = alpha_new/(alpha_new + beta_new)
true_var = (alpha_new*beta_new) / ((alpha_new + beta_new)^2 * (alpha_new + beta_new +1))
true_sd = sqrt(true_var)

```


```{r}
# calculate how the mean changes with increasing number of n - until 10000 values
set.seed(12345)
iterations = 1:1000
mean_of_n = c()
for (i in 1:length(iterations)) {
  mean_of_n[i] = mean(rbeta(n = i, shape1 = alpha_new, shape2 = beta_new))
}

# calculate how the sd changes with increasing number of n 
sd_of_n = c()
for (i in 1:length(iterations)) {
  sd_of_n[i] = sd(rbeta(n = i, shape1 = alpha_new, shape2 = beta_new))
}
```



```{r, warning=FALSE, echo=FALSE}
# create plot data
plot_data1.a = data.frame(x = iterations,
                          true_mean = true_mean,
                          true_sd = true_sd)

# create converges plot for mean and sd
mean_converges = ggplot(data = plot_data1.a, aes(x = plot_data1.a$x)) + 
  geom_point(aes(x = x, y = mean_of_n, colour = "Samples")) +
  geom_hline(aes(yintercept = true_mean, colour = "True mean")) + 
  ggtitle("mean converges to the true values") + xlab("n") + ylab("y")

sd_converges = ggplot(data = plot_data1.a, aes(x = plot_data1.a$x)) + 
  geom_point(aes(x = x, y = sd_of_n, colour = "Samples"))+
  geom_hline(aes(yintercept = true_sd,colour = "True sd")) +
  ggtitle("standard deviation converges to the true values") + xlab("n") + ylab("y")

grid.arrange(mean_converges, sd_converges, nrow = 2)

```


## (b) Use simulation (nDraws = 10000) to compute the posterior probability 
Pr($\theta < 0.4 \mid y)$ and compare with the exact value [Hint: pbeta()].
```{r, echo=FALSE, eval=FALSE}
# Example code for the function pnorm
# What is P(Z > 1.83)?
pnorm(1.83, mean=0, sd = 1, lower=F)
# What is P(Z < -0.42)?
pnorm(-0.42, mean=0, sd = 1, lower=T)
```


```{r}
set.seed(12345)
# simulation to compute the posterior probability - of beta theata < 0.4 
nDraws_b = 10000
sample_b = rbeta(n = nDraws_b, shape1 = 16, shape2 = 8)
sample_b_binary = ifelse(sample_b < 0.4, 1, 0) # if value in sample is smaler than 0, than 1 else 0
prob_sample_b = sum(sample_b_binary)/nDraws_b

# exact value theta < 0.4
exact_value_1b  = pbeta(q = 0.4, shape1 = alpha_new, shape2 = beta_new)
exact_value_1b = round(exact_value_1b, 4)

```

```{r, echo=FALSE}
# result table 
result_1b_data = data.frame("Expected value" = exact_value_1b,
                            "Simulated vaule" = prob_sample_b)
result_1b_data
```


##(c) Compute the posterior distribution of the log-odds 
$\phi = log(\frac{\theta}{1-\theta})$(nDraws = 10000). 
[Hint: hist() and density() might come in handy]
```{r}
set.seed(12345)
nDraws_c = 10000
sample_c = rbeta(n = nDraws_b, shape1 = 16, shape2 = 8)
log_odds = log(sample_c/(1-sample_c))

hist(log_odds, probability = TRUE)
lines(density(log_odds)) # run all lines at the same time to create the plot


```


# 2. Log-normal distribution and the Gini coefficient.
Assume that you have asked 10 randomly selected persons about their monthly in- come (in thousands Swedish Krona) and obtained the following ten observations: 14, 25, 45, 25, 30, 33, 19, 50, 34 and 67. A common model for non-negative continuous variables is the log-normal distribution. The log-normal distribution logN($\mu, \sigma^2$) has density function

$$ p(y\mid \mu, \sigma^2) = \frac{1}{y \sqrt{2\pi\sigma^2}} exp\big[ - \frac{1}{2\sigma^2}(log \  y- \mu^2) \big]  $$

for $y > 0, \mu > 0$. The log-normal distribution is related to the normal distribution as follows: if $ y \sim log\  \mathcal{N}(\mu, \sigma^2)$ then $log\ y \sim \mathcal{N}(\mu, \sigma^2)$. Let  2iid 2
y1,...,yn|μ,σ ∼ logN(μ,σ ), where μ = 3.5 is assumed to be known but σ is
2 unknown with non-informative prior p(σ2) ∝ 1/σ2. The posterior for σ2 is the
Inv−χ2(n,τ2) distribution, where


a)
```{r, echo=FALSE}
# given values
observations = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)
n_observations = length(observations)
nDraws_2a = 10000
mu = 3.5
```

```{r}
# --- theoreticle value 
chi_squared = function(y,mu = 3.5){
  n = length(y)
  result = sum((log(y)-mu)^2)/n
  return(result)
}

t_2 = chi_squared(observations)

# theoreticle mean
theoreticle_mean = n_observations * t_2/(n_observations-2)

# theoreticle var
theoreticle_var = 2 * n_observations^2 * t_2^2 / ((n_observations -2)^2 * (n_observations - 4))

# --- simulate values 
set.seed(12345)
nDraws_2b = 10000
computed_variance = c()
for (i in 1:nDraws_2b) {
  X = rchisq(1, n_observations)
  computed_variance[i] =  (n_observations) * t_2 / X
}

simulated_mean = mean(computed_variance)
simulated_var = var(computed_variance)

result_2a_data = data.frame("theoreticle value" = c(theoreticle_mean,theoreticle_var),
                            "simulated value" = c(simulated_mean, simulated_var))
rownames(result_2a_data) = c("mean", "variance")
result_2a_data

```

b)
```{r, warning=FALSE}
# Gini coefficient G
# phi(z) - CDF for standard normal distribution - mu = 0, unit variance
# posterior drawn in a - computed_variance 
# compute  posterior distribution of the Gini coefficient 
G = 2 * pnorm(mean = 0, sqrt(computed_variance)/sqrt(2)) -1 


# create histogramm
#hist(G)

# plot data
plot_data2.b = data.frame("G" = G,
                          "nr" = 1:length(G))
# Histogram
ggplot(data = plot_data2.b, aes(x=G)) + 
  geom_histogram() + ggtitle("Histogram of the posterior distribution of the Gini coefficient")
```

c)
```{r}

```



#3 
a)
```{r}
# given values 
degrees = c(40, 303, 326, 285, 296, 314, 20, 308, 299, 296)
n_degrees = length(degrees)
radians = c(-2.44, 2.14, 2.54, 1.83, 2.02, 2.33, -2.79, 2.23, 2.07, 2.02)
n_radians = length(radians)
mu_3a = 2.39
# grid of k values
k = seq(from = 0.1,to = 8, by = 0.01) # k>0
```


```{r}
# calcualte the posterior 
# calculateion 1
result_3a = exp(k*sum(cos(radians-mu_3a))-k)/besselI(x = k, nu=0)^n_radians

# calculation 2
#result_3a = exp(k*(sum(cos(radians-mu_3a))-1))/besselI(x = k, nu=0)^n_radians

```

```{r, echo=FALSE, warning=FALSE}
# create plot data
plot_data3.a = data.frame("k" = k,
                          "posterior" = result_3a)
# histogram
# ggplot(data = plot_data3.a, aes(x=result_3a_v2)) + 
#   geom_histogram() 

plot_3a = ggplot(data = plot_data3.a, aes(x=k, y=result_3a)) + 
  geom_line()

# find the mode
#k_max = max(result_3a)

k_max_index = which.max(result_3a)
k_max = result_3a[k_max_index]

plot_3a + geom_vline(aes(xintercept = (k_max_index*0.01 + 0.01)))


```


b)

