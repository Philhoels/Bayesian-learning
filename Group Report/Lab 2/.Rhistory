sigma2_0 <- 1
# linear model - without beta
lmTemp = lm( temp ~ time + I(time^2), data = tempdata)
summary(lmTemp)
# plot the data
ggplot(tempdata, aes(x = time, y=temp)) +
geom_point(color = "#00BFC4") +                         # default blue
ylab("Temperature") + xlab("Time")
# create simulation
set.seed(123456)
# joint conjugate prior
N <- 40  # nr of simulations
# data frame to save prior coef data
prior <- matrix(ncol = 3, nrow = N)
for (i in 1:N) {
# lecture 5 slide 7 - Inv - chi^2(v_0,sigma^2_0)
var  <- LaplacesDemon::rinvchisq(1 ,v_0, sigma2_0)
# solve(A)	Inverse of A where A is a square matrix
beta <- MASS::mvrnorm(1, mu_0, var*solve(omega_0))
prior[i,1:3] <- beta
}
data.1a <- as.data.frame(cbind(tempdata$time, X%*%t(prior)))
cnames <- c("x")
for (i in 1:N) {
cnames[1+i] <- paste0("pred.",i)
}
colnames(data.1a) <- cnames
data.1a <- melt(data.1a, id.vars = "x")
plot1a.ori <- ggplot(data.1a)+
geom_line(aes(x = x, y = value, color = variable)) +
geom_point(data = tempdata, aes(x = time, y = temp), color = "#00BFC4") +
ggtitle("Original") +
theme(legend.position = "none")
# modify the parameters
mu_0     <- c(-11, 104, -98)  # c(-10, 100, -100)
omega_0  <- 0.01 * diag(3)
v_0      <- 4
sigma2_0 <- 0.02              # 1
prior <- matrix(ncol = 3, nrow = N)
for (i in 1:N) {
var  <- LaplacesDemon::rinvchisq(1 ,v_0, sigma2_0)
beta <- MASS::mvrnorm(1, mu_0, var*solve(omega_0))
prior[i,1:3] <- beta
}
data.1a <- as.data.frame(cbind(tempdata$time, X%*%t(prior)))
cnames <- c("x")
for (i in 1:N) {
cnames[1+i] <- paste0("pred.",i)
}
colnames(data.1a) <- cnames
data.1a <- melt(data.1a, id.vars = "x")
plot1a.mdf <- ggplot(data.1a)+
geom_line(aes(x = x, y = value, color = variable)) +
geom_point(data = tempdata, aes(x = time, y = temp), color = "#00BFC4") +
ggtitle("Modified") +
theme(legend.position = "none")
plot(arrangeGrob(plot1a.ori, plot1a.mdf, nrow = 2))
# posterior
k        <- 3 #  nr of regression coefficients
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
mu_n     <- solve(t(X) %*% X + omega_0) %*% (t(X) %*% X %*% beta_hat + omega_0 %*% mu_0)
omega_n    <- t(X) %*% X+omega_0
v_n      <- v_0 + n
sigma2_n <- (v_0 * sigma2_0 + (t(y) %*% y + t(mu_0) %*% omega_0 %*% mu_0 - t(mu_n) %*% omega_n %*% mu_n)) / v_n
# marginal posterior
###### Zijie idea:
# The arguments of marginal posterior of conjugate prior
# are guessed form marginal posterior of uniform prior (page 6)
######
data_hist <- as.data.frame(
mvtnorm::rmvt(n = 100, delta = mu_n, df = n-k,
sigma = as.numeric(sigma2_n) * solve(omega_n))
)
cnames <- c("beta0", "beta1", "beta2")
colnames(data_hist) <- cnames
f <- function(cname){
ggplot(data_hist, aes_string(x=cname)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666")
}
plot(arrangeGrob(grobs = lapply(cnames, f)))
# overlay a curve for the posterior median
beta_median = matrixStats::colMedians(as.matrix(data_hist))
pred.1b = beta_median %*% t(X)
# credible intervals
# each column represents all possible values at a fixed time
preds <- as.matrix(data_hist) %*% t(X)
pred_interval <- data.frame(nrow = n, nrow = 2)
colnames(pred_interval) <- c("lower","upper")
for(i in 1:n){
data_t <- preds[,i]
pred_interval[i,] <- quantile(data_t, probs = c(0.025,0.975))
}
data.1b <- cbind(tempdata, t(pred.1b), pred_interval)
ggplot(data.1b) +
geom_point(aes(x = time, y = temp), color = "#00BFC4") +
geom_line(aes(x = time, y = t(pred.1b)),color = "Blue",size = 1) +
geom_line(aes(x = time, y = lower), color = "Red", linetype = "dotted", size = 1.5) +
geom_line(aes(x = time, y = upper), color = "Red", linetype = "dotted", size = 1.5)
pred_highest <- c()
for(i in 1:366){
pred_highest[i] <- max(preds[,i])
}
data.1c <- cbind(tempdata, t(pred.1b), pred_interval, pred_highest)
ggplot(data.1c) +
geom_point(aes(x = time, y = temp), color = "#00BFC4") +
geom_line(aes(x = time, y = t(pred.1b)),color = "Blue",size = 1) +
geom_line(aes(x = time, y = lower), color = "Red", linetype = "dotted", size = 1.5) +
geom_line(aes(x = time, y = upper), color = "Red", linetype = "dotted", size = 1.5) +
geom_line(aes(x = time,y = pred_highest), color = "black", linetype = "solid", size = 1)
# clean up environment
rm(list=ls())
knitr::include_graphics("lab2_table_ex2.png")
knitr::opts_chunk$set(echo = TRUE, out.width = "400px")
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
# load the data & define y (response) and x 8 dimensional vector of the features
WomenWork = read.table("WomenWork.dat", header = TRUE)
y = WomenWork$Work
X = WomenWork[,2:9]
X = as.matrix(X)
n = nrow(WomenWork) # nr of observations
# logistic regression
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
summary(glmModel)
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2*diag(m)  # Prior variance matrix
# Defining the functions that returns the log posterior (Logistic models).
# Note that the first input argument of this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
OptimResults <- optim(initVal,
fn = LogPostLogistic,
y, X, mu, Sigma,
method  = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2*diag(m)  # Prior variance matrix
# Defining the functions that returns the log posterior (Logistic models).
# Note that the first input argument of this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
browser()
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
OptimResults <- optim(initVal,
fn = LogPostLogistic,
y, X, mu, Sigma,
method  = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
y
# Printing the results to the screen
postMode <- OptimResults$par
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2*diag(m)  # Prior variance matrix
# Defining the functions that returns the log posterior (Logistic models).
# Note that the first input argument of this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
browser()
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
OptimResults <- optim(initVal,
fn = LogPostLogistic,
y = y,
X = X,
MU = mu,
Sigma = Sigma,
method = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
knitr::opts_chunk$set(echo = TRUE, out.width = "400px")
# packages we use in the lab
library(mvtnorm)
library(LaplacesDemon)
library(ggplot2)
library(reshape2)
library(gridExtra)
knitr::include_graphics("lab2_table_ex2.png")
# load the data & define y (response) and x 8 dimensional vector of the features
WomenWork = read.table("WomenWork.dat", header = TRUE)
y = WomenWork$Work
X = WomenWork[,2:9]
X = as.matrix(X)
n = nrow(WomenWork) # nr of observations
# logistic regression
glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)
summary(glmModel)
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2*diag(m)  # Prior variance matrix
# Defining the functions that returns the log posterior (Logistic models).
# Note that the first input argument of this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
browser()
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
OptimResults <- optim(initVal,
fn = LogPostLogistic,
y = y,
X = X,
MU = mu,
Sigma = Sigma,
method = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2*diag(m)  # Prior variance matrix
# Defining the functions that returns the log posterior (Logistic models).
# Note that the first input argument of this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
OptimResults <- optim(initVal,
LogPostLogistic,
y = y,
X = X,
MU = mu,
Sigma = Sigma,
method = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
# parameters
mu  <- 0
tau <- 10  # Prior scaling factor such that Prior Covariance = (tau^2)*I - need tau to define sigma later on
# rename X columns
covNames <- names(WomenWork)[2:length(names(WomenWork))]
m <- dim(X)[2]
# setting up the prior
mu    <- as.vector(rep(mu,m))  # Prior mean vector
Sigma <- tau^2*diag(m)  # Prior variance matrix
# Defining the functions that returns the log posterior (Logistic models).
# Note that the first input argument of this function must be the one that we optimize on, i.e. the regression coefficients.
LogPostLogistic <- function(beta, y, X, mu, Sigma){
M <- length(beta)
p <- X %*% beta
# log the likelihood of Binomial Distribution
logLik <- sum(p * y - log(1 + exp(p)))
if (abs(logLik) == Inf){
logLik <- -50000
# Likelihood is not finite, stear the optimizer away from here!
}
# prior follows multi-normal distribution with
logPrior <- dmvnorm(beta, mu, Sigma, log = TRUE)
# cuz we logarithmize the likelihood and prior,
# posterior is the sum of them
return(logLik + logPrior)
}
# use random initial values
initVal <- as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS
OptimResults <- optim(initVal,
LogPostLogistic,
y = y,
X = X,
mu = mu,
Sigma = Sigma,
method = c("BFGS"),
control = list(fnscale=-1),
hessian = TRUE)  # output hessian matrix
# Printing the results to the screen
postMode <- OptimResults$par
postCov <- -solve(OptimResults$hessian) # Posterior covariance matrix is -inv(Hessian)
names(postMode) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(postCov)) # Computing approximate standard deviations.
names(approxPostStd) <- covNames # Naming the coefficient by covariates
print('The posterior mode is:')
print(postMode)
print('The approximate posterior standard deviation is:')
print(approxPostStd)
OptimResults
postMode %*% t(X)
postMode %*% t(X) > 0
WomenWork[,7]
WomenWork$NSmallChild
WomenWork[,8]
WomenWork[,1]
data.2b <- WomenWork[,c(1,8)]
plot(ddata.2b)
plot(data.2b)
View(data.2b)
plot(ddata.2b, x=NSmallChild, y= Work)
plot(data.2b, x=NSmallChild, y= Work)
plot(x=data.2b$NSmallChild, y= data.2b$Work)
data.2b <- WomenWork[,c(1,8)]
dim(postMode)
postMode
data.beta <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)[,7]
quantile(data.beta, probs = c(0.025,0.975))
postMode[7]+approxPostStd[7]
postMode[7]-approxPostStd[7]
postMode[7]-1.96*approxPostStd[7]
# Compute an approximate 95% credible interval for the variable NSmallChild
data.NSmallChild <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)[,7]
quantile(data.NSmallChild, probs = c(0.025,0.975))
c(postMode[7]-1.96*approxPostStd[7], postMode[7]+1.96*approxPostStd[7])
postMode[7]
postMode$NSmallChild
# By mean and standard deviation
interval <- c(postMode[7]-1.96*approxPostStd[7], postMode[7]+1.96*approxPostStd[7])
colnames(interval) <- NULL
interval
colnames(interval) <- c(1,1)
colnames(interval)
interval
names(interval) <- c(1,1)
interval
# Compute an approximate 95% credible interval for the variable NSmallChild
# By simulation
data.NSmallChild <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)[,7]
quantile(data.NSmallChild, probs = c(0.025,0.975))
# By mean and standard deviation
interval <- c(postMode[7]-1.96*approxPostStd[7], postMode[7]+1.96*approxPostStd[7])
names(interval) <- NULL
interval
# Compute an approximate 95% credible interval for the variable NSmallChild
# By simulation
data.NSmallChild <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)[,7]
quantile(data.NSmallChild, probs = c(0.025,0.975))
# By mean and standard deviation
interval <- c(postMode[7]-1.96*approxPostStd[7], postMode[7]+1.96*approxPostStd[7])
names(interval) <- NULL
cat(interval)
data_hist <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)
colnames(data_hist) <- covNames
f <- function(cname){
ggplot(data_hist, aes_string(x=cname)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666")
}
plot(arrangeGrob(grobs = lapply(covNames, f)))
covNames
example <- matrix(c(10, 8, 10, 10**2, 40, 1, 1), nrow = 1)
example
X
example <- matrix(c(10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
pred.hist <- data.hist %*% example
data.hist <- as.data.frame(
mvtnorm::rmvnorm(n = 1000, mean = postMode, sigma = postCov)
)
colnames(data.hist) <- covNames
f <- function(cname){
ggplot(data.hist, aes_string(x=cname)) +
geom_histogram(aes(y = ..density..),
colour = "black",
fill   = "white",
bins   = 30) +
geom_density(alpha = .2, fill = "#FF6666")
}
plot(arrangeGrob(grobs = lapply(covNames, f)))
example <- matrix(c(10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
pred.hist <- data.hist %*% example
dim(data.hist)
pred.hist <- t(data.hist) %*% example
dim(t(data.hist))
dim(example)
example <- matrix(c(1, 10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
pred.hist <- t(data.hist) %*% example
example <- matrix(c(1, 10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
pred.hist <- t(data.hist) %*% example
dim(example)
dim(t(data.hist))
pred.hist <- example %*% t(data.hist)
pred.hist
hist(pred.hist)
class(pred.hist)
pred.hist <- as.data.frame(example %*% t(data.hist))
class(pred.hist)
pred.hist
pred.hist <- as.data.frame(t(example %*% t(data.hist)))
example <- matrix(c(1, 10, 8, 10, (10/10)^2, 40, 1, 1), nrow = 1)
pred.hist <- as.data.frame(t(example %*% t(data.hist)))
ggplot(pred.hist, aes(x=V1)) +
geom_density()
ggplot(pred.hist, aes(x=V1)) +
geom_density(alpha = .2, fill = "#FF6666")
